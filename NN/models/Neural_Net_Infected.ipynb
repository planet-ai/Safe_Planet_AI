{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic PyTorch Neural Network\n",
    "---\n",
    "## What will we work with ? \n",
    "---\n",
    "Time to put the pieces together. In this sections we'll :\n",
    "- Creat a multi-layer deep learning model\n",
    "- Load data\n",
    "- Train and Validate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To - Do\n",
    "---\n",
    "\n",
    "1. Should I add a third layer ?\n",
    "   <br>After adding third layer, we got to know that we can achieve low training error for some cases. However, the testing RMSE for those cases remains high. \n",
    "\n",
    "For the ones where testing RMSE is low, training RMSE is high.\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Standard Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Model Class \n",
    "---\n",
    "## Activation : Leaky Relu for L1 | Relu for L2 | \n",
    "---\n",
    "\n",
    "| Input Layer | Layer 1 | Layer 2 | Output Layer | TEST_RMSE |\n",
    "| ----------- | ------- | ------- | ------------ | --------- |\n",
    "| 10 | 100 | 2 | 1 | 38.23873138 |\n",
    "| 10 | 50 | 25 | 1 | 2232.96899414 |\n",
    "| 10 | 60 | 20 | 1 | 2277.70410156 |\n",
    "| 10 | 40 | 20 | 1 | 2330.19702148 |\n",
    "| 10 | 20 | 30 | 1 | 2262.53344727 |\n",
    "| 10 | 30 | 20 | 1 | 2346.64111328 |\n",
    "| 10 | 200 | 100 | 1 | 2408.80444336 |\n",
    "| 10 | 35 | 15 | 1 | 2238.07788086 |\n",
    "| 10 | 30 | 2 | 1 | 46.84243011 |\n",
    "| 10 | 50 | 10 | 1 | 1894.40747070 |\n",
    "\n",
    "---\n",
    "\n",
    "## Activation : Leaky Relu for L1 | Relu for L2 | Relu for L3 |\n",
    "---\n",
    "\n",
    "| Layer 1 | Layer 2 | Layer 3 | TEST_RMSE |\n",
    "| ----------- | ------- | ------- | ------------ |\n",
    "| 100 | 50 | 2  | 39.08463287 |\n",
    "| 30  | 20 | 10 | 1057.11279297 |\n",
    "| 100 | 20 | 10 | 2224.03369141 |\n",
    "| 20  | 6  | 2  | 38.90698242   |\n",
    "| 20  | 6  | 1  | 2256.34155273 |\n",
    "| 30  | 10 | 2  | 2717.19726562 | (good training rmse)\n",
    "| 30  | 11 | 3  | 2684.04199219 | (good training rmse)\n",
    "| 30  | 11 | 5  | 2796.58154297 | (good training rmse)\n",
    "| 50  | 11 | 5  | 2818.67822266 | (good training rmse)\n",
    "| 30  | 20 | 2  | 146.12924194  |\n",
    "| 21  | 11 | 3  | 2631.75976562 | (v. good training rmse)\n",
    "| 21  | 10 | 3  | 2739.56372070 | (good training rmse)\n",
    "| 200 | 44 | 10 | 2683.55517578 | (training rmse : 300)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Number of Layers and Activation Function (Finalized)\n",
    "## L1 = 66 | L2 = 44 | L3 = 22\n",
    "## L1 = 200 | L2 = 44 | L3 = 10\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers in L 1 : 200\n",
      "Number of Layers in L 2 : 44\n",
      "Number of Layers in L 3 : 10\n"
     ]
    }
   ],
   "source": [
    "l1 = input(\"Number of Layers in L 1 : \")\n",
    "l2 = input(\"Number of Layers in L 2 : \")\n",
    "l3 = input(\"Number of Layers in L 3 : \")\n",
    "# n_iter = input(\"Number of Epochs : \")\n",
    "\n",
    "l1 = int(l1)\n",
    "l2 = int(l2)\n",
    "l3 = int(l3)\n",
    "# n_iter = int(n_iter)\n",
    "n_iter = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=10, l1=l1, l2=l2, l3=l3, out_features=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.fc1 = nn.Linear(in_features, l1)\n",
    "        # hidden layer 1\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        # hidden layer 2\n",
    "        self.fc3 = nn.Linear(l2, l3)\n",
    "        #output layer\n",
    "        self.out = nn.Linear(l3, out_features)\n",
    "        \n",
    "          \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = (self.fc3(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model class using parameter defaults\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(44)\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = pd.read_csv('../data_data/train_infections.csv')\n",
    "test_d = pd.read_csv('../data_data/test_infections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_d.iloc[:,0:10]\n",
    "y_train = train_d.iloc[:,10:11]\n",
    "\n",
    "X_test = test_d.iloc[:,0:10]\n",
    "y_test = test_d.iloc[:,0:10:11]\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataLoader\n",
    "---\n",
    "For this analysis we don't need to create a Dataset object. <br>\n",
    "But we should take advantage of PyTorch's DataLoader tool. <br>\n",
    "Even though our dataset is small, we'll load it into our model in two batches. <br>\n",
    "This technique becomes very helpful with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(X_train, batch_size=100, shuffle=True)\n",
    "\n",
    "testloader = DataLoader(X_test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Redo\n",
    "torch.manual_seed(44)\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  loss: 8657997.00000000\n",
      "epoch: 11  loss: 3011211.50000000\n",
      "epoch: 21  loss: 3256908.00000000\n",
      "epoch: 31  loss: 2924596.75000000\n",
      "epoch: 41  loss: 2908815.25000000\n",
      "epoch: 51  loss: 2867730.75000000\n",
      "epoch: 61  loss: 2858215.75000000\n",
      "epoch: 71  loss: 2839967.25000000\n",
      "epoch: 81  loss: 2824086.00000000\n",
      "epoch: 91  loss: 2808145.25000000\n",
      "epoch: 101  loss: 2791290.50000000\n",
      "epoch: 111  loss: 2773702.75000000\n",
      "epoch: 121  loss: 2755039.00000000\n",
      "epoch: 131  loss: 2734512.75000000\n",
      "epoch: 141  loss: 2710903.25000000\n",
      "epoch: 151  loss: 2684885.25000000\n",
      "epoch: 161  loss: 2657677.00000000\n",
      "epoch: 171  loss: 2630458.50000000\n",
      "epoch: 181  loss: 2604459.50000000\n",
      "epoch: 191  loss: 2580463.75000000\n",
      "epoch: 201  loss: 2556786.00000000\n",
      "epoch: 211  loss: 2533231.50000000\n",
      "epoch: 221  loss: 2509760.50000000\n",
      "epoch: 231  loss: 2486358.00000000\n",
      "epoch: 241  loss: 2463153.00000000\n",
      "epoch: 251  loss: 2440203.75000000\n",
      "epoch: 261  loss: 2418356.75000000\n",
      "epoch: 271  loss: 2397068.25000000\n",
      "epoch: 281  loss: 2376213.75000000\n",
      "epoch: 291  loss: 2355677.50000000\n",
      "epoch: 301  loss: 2335649.50000000\n",
      "epoch: 311  loss: 2316639.75000000\n",
      "epoch: 321  loss: 2298248.50000000\n",
      "epoch: 331  loss: 2280458.75000000\n",
      "epoch: 341  loss: 2263057.00000000\n",
      "epoch: 351  loss: 2246021.25000000\n",
      "epoch: 361  loss: 2229195.50000000\n",
      "epoch: 371  loss: 2212630.75000000\n",
      "epoch: 381  loss: 2196355.25000000\n",
      "epoch: 391  loss: 2180312.25000000\n",
      "epoch: 401  loss: 2164510.75000000\n",
      "epoch: 411  loss: 2148794.50000000\n",
      "epoch: 421  loss: 2116465.50000000\n",
      "epoch: 431  loss: 2095968.75000000\n",
      "epoch: 441  loss: 2074546.25000000\n",
      "epoch: 451  loss: 2050891.12500000\n",
      "epoch: 461  loss: 2027738.87500000\n",
      "epoch: 471  loss: 2004310.00000000\n",
      "epoch: 481  loss: 1980504.37500000\n",
      "epoch: 491  loss: 1955905.75000000\n",
      "epoch: 501  loss: 1930637.00000000\n",
      "epoch: 511  loss: 1904599.87500000\n",
      "epoch: 521  loss: 1877481.62500000\n",
      "epoch: 531  loss: 1849121.75000000\n",
      "epoch: 541  loss: 1819718.37500000\n",
      "epoch: 551  loss: 1789078.87500000\n",
      "epoch: 561  loss: 1757499.50000000\n",
      "epoch: 571  loss: 1725161.87500000\n",
      "epoch: 581  loss: 1692033.37500000\n",
      "epoch: 591  loss: 1657916.25000000\n",
      "epoch: 601  loss: 1622746.87500000\n",
      "epoch: 611  loss: 1586635.37500000\n",
      "epoch: 621  loss: 1549480.00000000\n",
      "epoch: 631  loss: 1511512.00000000\n",
      "epoch: 641  loss: 1472623.62500000\n",
      "epoch: 651  loss: 1432821.75000000\n",
      "epoch: 661  loss: 1392069.50000000\n",
      "epoch: 671  loss: 1350699.25000000\n",
      "epoch: 681  loss: 1308555.12500000\n",
      "epoch: 691  loss: 1265922.75000000\n",
      "epoch: 701  loss: 1222765.12500000\n",
      "epoch: 711  loss: 1179170.12500000\n",
      "epoch: 721  loss: 1135333.00000000\n",
      "epoch: 731  loss: 1090934.62500000\n",
      "epoch: 741  loss: 1046423.81250000\n",
      "epoch: 751  loss: 1002081.25000000\n",
      "epoch: 761  loss: 957614.75000000\n",
      "epoch: 771  loss: 913314.25000000\n",
      "epoch: 781  loss: 869339.43750000\n",
      "epoch: 791  loss: 825823.50000000\n",
      "epoch: 801  loss: 783007.25000000\n",
      "epoch: 811  loss: 740910.93750000\n",
      "epoch: 821  loss: 699807.75000000\n",
      "epoch: 831  loss: 659832.12500000\n",
      "epoch: 841  loss: 522408.65625000\n",
      "epoch: 851  loss: 453349.78125000\n",
      "epoch: 861  loss: 411346.59375000\n",
      "epoch: 871  loss: 375853.96875000\n",
      "epoch: 881  loss: 345004.03125000\n",
      "epoch: 891  loss: 317644.62500000\n",
      "epoch: 901  loss: 293106.06250000\n",
      "epoch: 911  loss: 271405.50000000\n",
      "epoch: 921  loss: 252062.45312500\n",
      "epoch: 931  loss: 235677.42187500\n",
      "epoch: 941  loss: 220110.42187500\n",
      "epoch: 951  loss: 206914.84375000\n",
      "epoch: 961  loss: 195235.71875000\n",
      "epoch: 971  loss: 185160.57812500\n",
      "epoch: 981  loss: 175396.67187500\n",
      "epoch: 991  loss: 167504.95312500\n",
      "epoch: 1001  loss: 159310.56250000\n",
      "epoch: 1011  loss: 152741.65625000\n",
      "epoch: 1021  loss: 147185.89062500\n",
      "epoch: 1031  loss: 142301.42187500\n",
      "epoch: 1041  loss: 139517.62500000\n",
      "epoch: 1051  loss: 134971.35937500\n",
      "epoch: 1061  loss: 128199.22656250\n",
      "epoch: 1071  loss: 125000.39843750\n",
      "epoch: 1081  loss: 122579.23437500\n",
      "epoch: 1091  loss: 120364.60937500\n",
      "epoch: 1101  loss: 118777.32031250\n",
      "epoch: 1111  loss: 116086.02343750\n",
      "epoch: 1121  loss: 114573.39843750\n",
      "epoch: 1131  loss: 112690.82812500\n",
      "epoch: 1141  loss: 113323.00781250\n",
      "epoch: 1151  loss: 111087.50781250\n",
      "epoch: 1161  loss: 109184.10937500\n",
      "epoch: 1171  loss: 108307.31250000\n",
      "epoch: 1181  loss: 107476.74218750\n",
      "epoch: 1191  loss: 106630.07812500\n",
      "epoch: 1201  loss: 105955.21093750\n",
      "epoch: 1211  loss: 105047.17968750\n",
      "epoch: 1221  loss: 104489.28125000\n",
      "epoch: 1231  loss: 104610.28125000\n",
      "epoch: 1241  loss: 103345.00000000\n",
      "epoch: 1251  loss: 102889.78125000\n",
      "epoch: 1261  loss: 102290.60156250\n",
      "epoch: 1271  loss: 101935.59375000\n",
      "epoch: 1281  loss: 101629.08593750\n",
      "epoch: 1291  loss: 101176.59375000\n",
      "epoch: 1301  loss: 100713.38281250\n",
      "epoch: 1311  loss: 100258.42968750\n",
      "epoch: 1321  loss: 100128.89062500\n",
      "epoch: 1331  loss: 100535.62500000\n",
      "epoch: 1341  loss: 99464.25781250\n",
      "epoch: 1351  loss: 99310.44531250\n",
      "epoch: 1361  loss: 98897.09375000\n",
      "epoch: 1371  loss: 98660.00000000\n",
      "epoch: 1381  loss: 98372.83593750\n",
      "epoch: 1391  loss: 97999.07031250\n",
      "epoch: 1401  loss: 98008.64062500\n",
      "epoch: 1411  loss: 99226.03906250\n",
      "epoch: 1421  loss: 98076.06250000\n",
      "epoch: 1431  loss: 97501.14062500\n",
      "epoch: 1441  loss: 97131.15625000\n",
      "epoch: 1451  loss: 96754.95312500\n",
      "epoch: 1461  loss: 96623.68750000\n",
      "epoch: 1471  loss: 96598.19531250\n",
      "epoch: 1481  loss: 96242.17968750\n",
      "epoch: 1491  loss: 96313.07812500\n",
      "epoch: 1501  loss: 96550.03906250\n",
      "epoch: 1511  loss: 95821.74218750\n",
      "epoch: 1521  loss: 95867.72656250\n",
      "epoch: 1531  loss: 95435.04687500\n",
      "epoch: 1541  loss: 95740.60156250\n",
      "epoch: 1551  loss: 95227.90625000\n",
      "epoch: 1561  loss: 95000.21875000\n",
      "epoch: 1571  loss: 94930.24218750\n",
      "epoch: 1581  loss: 94715.63281250\n",
      "epoch: 1591  loss: 94542.70312500\n",
      "epoch: 1601  loss: 95038.14062500\n",
      "epoch: 1611  loss: 95378.25000000\n",
      "epoch: 1621  loss: 94747.25000000\n",
      "epoch: 1631  loss: 94223.50781250\n",
      "epoch: 1641  loss: 94056.53906250\n",
      "epoch: 1651  loss: 93930.85937500\n",
      "epoch: 1661  loss: 93811.88281250\n",
      "epoch: 1671  loss: 93679.49218750\n",
      "epoch: 1681  loss: 93718.60937500\n",
      "epoch: 1691  loss: 93426.06250000\n",
      "epoch: 1701  loss: 93316.47656250\n",
      "epoch: 1711  loss: 93155.04687500\n",
      "epoch: 1721  loss: 93124.12500000\n",
      "epoch: 1731  loss: 92962.73437500\n",
      "epoch: 1741  loss: 92832.39062500\n",
      "epoch: 1751  loss: 93778.67968750\n",
      "epoch: 1761  loss: 92615.29687500\n",
      "epoch: 1771  loss: 92555.35156250\n",
      "epoch: 1781  loss: 92518.80468750\n",
      "epoch: 1791  loss: 92436.59375000\n",
      "epoch: 1801  loss: 92288.88281250\n",
      "epoch: 1811  loss: 92531.21875000\n",
      "epoch: 1821  loss: 92888.39843750\n",
      "epoch: 1831  loss: 92333.91406250\n",
      "epoch: 1841  loss: 91740.88281250\n",
      "epoch: 1851  loss: 91821.57031250\n",
      "epoch: 1861  loss: 91599.00781250\n",
      "epoch: 1871  loss: 91967.05468750\n",
      "epoch: 1881  loss: 92351.32031250\n",
      "epoch: 1891  loss: 91539.00781250\n",
      "epoch: 1901  loss: 91404.95312500\n",
      "epoch: 1911  loss: 91363.48437500\n",
      "epoch: 1921  loss: 90991.20312500\n",
      "epoch: 1931  loss: 90968.50781250\n",
      "epoch: 1941  loss: 90936.63281250\n",
      "epoch: 1951  loss: 92044.71093750\n",
      "epoch: 1961  loss: 90803.69531250\n",
      "epoch: 1971  loss: 90782.25781250\n",
      "epoch: 1981  loss: 90713.92968750\n",
      "epoch: 1991  loss: 90459.14843750\n"
     ]
    }
   ],
   "source": [
    "epochs = n_iter\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # a neat trick to save screen space:\n",
    "    if i%10 == 1:\n",
    "        print(f'epoch: {i:2}  loss: {loss.item():1.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 300.34558105\n"
     ]
    }
   ],
   "source": [
    "# TO EVALUATE THE ENTIRE TRAINING SET\n",
    "with torch.no_grad():\n",
    "    y_val = model(X_train)\n",
    "    loss = torch.sqrt(criterion(y_val, y_train))\n",
    "print(f'RMSE: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model\n",
    "---\n",
    "Here, we want to run the entire test set through the model, and compare it to the known value. <br>\n",
    "For this step, we don't want to update weights and biases, so we set torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2683.55517578\n"
     ]
    }
   ],
   "source": [
    "# TO EVALUATE THE ENTIRE TEST SET\n",
    "with torch.no_grad():\n",
    "    y_val = model(X_test)\n",
    "    loss = torch.sqrt(criterion(y_val, y_test))\n",
    "print(f'RMSE: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the R2 score for regression\n",
    "---\n",
    "#### Using scikit learn library to calculate this\n",
    "#### r2_score(y_true, y_pred)\n",
    "---\n",
    "\n",
    "## Note : This metrics shows goodness of fit for the regression model. We don't have a time series data in this model. So, it will be not useful use this metric for now. Might be useful once we use this model for a particular county with a time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850548186197268"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = r2(y_train, y_val)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bita9191fbd638c401c8851d16f8fd401f3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
